{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "yhK9FniwYfR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3O2t-SORG0r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Adafactor, AdamW"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your JSON file into a pandas dataframe\n",
        "df = pd.read_json('/content/drive/MyDrive/Assignments/Project/reviews.json', lines=True)\n",
        "# Combine summary and reviewText fields\n",
        "df['text'] = df['summary'].fillna('') + '. ' + df['reviewText'].fillna('')\n",
        "# sentiment polarity classification\n",
        "X_data = df['text']\n",
        "y_data = df['overall'].apply(lambda x: 'positive' if x > 3 else ('neutral' if x == 3 else 'negative'))\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'text': X_data,\n",
        "    'label': y_data\n",
        "})\n",
        "\n",
        "label_counts_df = df['label'].value_counts()\n",
        "\n",
        "print(label_counts_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "708nAVMSRUW5",
        "outputId": "42b29609-7649-4ed9-94e0-bd4f4701e457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive    33516\n",
            "negative    11530\n",
            "neutral      4954\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(n=1000, random_state=42)"
      ],
      "metadata": {
        "id": "894ThefTkV_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the DataFrame into training and test sets (80:20)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "# Assuming you have process_data function and train_df\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2)"
      ],
      "metadata": {
        "id": "FB7eT1fvR0-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "# Function to encode data\n",
        "def process_data(df):\n",
        "    input_texts = [\"review: \" + text for text in df['text'].values]\n",
        "    target_texts = [str(label) for label in df['label'].values]\n",
        "\n",
        "    input_encodings = tokenizer(input_texts, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
        "    target_encodings = tokenizer(target_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "    return input_encodings.input_ids, target_encodings.input_ids\n",
        "\n",
        "# Prepare data\n",
        "train_input_ids, train_target_ids = process_data(train_df)\n",
        "test_input_ids, test_target_ids = process_data(test_df)\n",
        "\n",
        "# Prepare data loaders\n",
        "train_dataset = TensorDataset(train_input_ids, train_target_ids)\n",
        "test_dataset = TensorDataset(test_input_ids, test_target_ids)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move data to GPU\n",
        "        if torch.cuda.is_available():\n",
        "            input_ids = batch[0].to(\"cuda\")\n",
        "            target_ids = batch[1].to(\"cuda\")\n",
        "        else:\n",
        "            input_ids = batch[0]\n",
        "            target_ids = batch[1]\n",
        "\n",
        "        outputs = model(input_ids=input_ids, labels=target_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfWF43pQrVuD",
        "outputId": "65bbd74f-1a4f-41b9-d0c4-f2aab596aa52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:26<00:00,  1.90it/s]\n",
            "100%|██████████| 50/50 [00:24<00:00,  2.03it/s]\n",
            "100%|██████████| 50/50 [00:24<00:00,  2.06it/s]\n",
            "100%|██████████| 50/50 [00:24<00:00,  2.03it/s]\n",
            "100%|██████████| 50/50 [00:24<00:00,  2.01it/s]\n",
            "100%|██████████| 50/50 [00:24<00:00,  2.03it/s]\n",
            "100%|██████████| 50/50 [00:24<00:00,  2.04it/s]\n",
            "100%|██████████| 50/50 [00:24<00:00,  2.03it/s]\n",
            "100%|██████████| 50/50 [00:24<00:00,  2.04it/s]\n",
            "100%|██████████| 50/50 [00:24<00:00,  2.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "for batch in tqdm(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = batch[0].to(\"cuda\")\n",
        "        target_ids = batch[1].to(\"cuda\")\n",
        "    else:\n",
        "        input_ids = batch[0]\n",
        "        target_ids = batch[1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids)\n",
        "\n",
        "    predictions = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in outputs]\n",
        "    targets = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in target_ids]\n",
        "\n",
        "    all_predictions.extend(predictions)\n",
        "    all_targets.extend(targets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN5a3l9GuiT4",
        "outputId": "3d51aaf2-2f26-4cbe-84c7-bff2056e0b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/13 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "100%|██████████| 13/13 [00:02<00:00,  5.13it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics\n",
        "print(\"Confusion Matrix:\", confusion_matrix(all_targets, all_predictions))\n",
        "print(\"Accuracy:\", accuracy_score(all_targets, all_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHkpWzV2tYCi",
        "outputId": "0f062319-b0f1-4a6e-ba4c-c4ae4e1cb96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix: [[ 25   1  19]\n",
            " [  1   6  16]\n",
            " [  1   1 130]]\n",
            "Accuracy: 0.805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For multi-class classification, the \"weighted\" average is generally used for precision, recall, and F1-score\n",
        "print(\"Precision:\", precision_score(all_targets, all_predictions, average='weighted'))\n",
        "print(\"Recall:\", recall_score(all_targets, all_predictions, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(all_targets, all_predictions, average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV2JLpLguoqI",
        "outputId": "1c1e0968-672f-4ea8-f4b6-94e9a94ff567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8145833333333333\n",
            "Recall: 0.805\n",
            "F1 Score: 0.7785439068100359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AUC calculation for multi-class sentiment analysis is a bit more involved and depends on one-vs-all binarization\n",
        "# Assuming all_targets and all_predictions contain string labels, you must first map them to integer indices\n",
        "unique_labels = np.unique(all_targets)\n",
        "targets_int = [np.where(label==unique_labels)[0][0] for label in all_targets]\n",
        "predictions_int = [np.where(label==unique_labels)[0][0] for label in all_predictions]\n",
        "\n",
        "# Then, binarize these integer indices\n",
        "targets_binarized = label_binarize(targets_int, classes=range(len(unique_labels)))\n",
        "predictions_binarized = label_binarize(predictions_int, classes=range(len(unique_labels)))\n",
        "\n",
        "# Compute AUC for each class and average\n",
        "auc = roc_auc_score(targets_binarized, predictions_binarized, multi_class='ovr', average='weighted')\n",
        "print(\"AUC:\", auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXSf4HPpivwt",
        "outputId": "6fa31806-17ef-44a0-8fe3-df002f06c8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.7305457284061794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process validation data like training and testing data\n",
        "val_input_ids, val_target_ids = process_data(val_df)\n",
        "val_dataset = TensorDataset(val_input_ids, val_target_ids)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n"
      ],
      "metadata": {
        "id": "Zl-6Hk98y-aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Function to encode data\n",
        "def process_data(df):\n",
        "    input_texts = [\"review: \" + text for text in df['text'].values]\n",
        "    target_texts = [str(label) for label in df['label'].values]\n",
        "\n",
        "    input_encodings = tokenizer(input_texts, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
        "    target_encodings = tokenizer(target_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "    return input_encodings.input_ids, target_encodings.input_ids\n",
        "\n",
        "# Prepare data\n",
        "input_ids, target_ids = process_data(df)  # Assume df is your DataFrame containing 'text' and 'label'\n",
        "\n",
        "# Prepare KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(input_ids)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    train_inputs, val_inputs = input_ids[train_index], input_ids[val_index]\n",
        "    train_targets, val_targets = target_ids[train_index], target_ids[val_index]\n",
        "\n",
        "    # Prepare data loaders\n",
        "    train_dataset = TensorDataset(train_inputs, train_targets)\n",
        "    val_dataset = TensorDataset(val_inputs, val_targets)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    # Initialize model for each fold\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.to(\"cuda\")\n",
        "\n",
        "    # Define Loss and Optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training loop for each fold\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        for batch in tqdm(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                input_ids = batch[0].to(\"cuda\")\n",
        "                target_ids = batch[1].to(\"cuda\")\n",
        "            else:\n",
        "                input_ids = batch[0]\n",
        "                target_ids = batch[1]\n",
        "\n",
        "            outputs = model(input_ids=input_ids, labels=target_ids)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "oi7Y_HGlzRh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics storage for each fold\n",
        "all_fold_metrics = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(input_ids)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "    # ... (previous training code)\n",
        "\n",
        "    # Initialize metrics for this fold\n",
        "    fold_metrics = {\n",
        "        'accuracy': [],\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'f1_score': []\n",
        "    }\n",
        "\n",
        "    # Validation loop for each fold\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for batch in tqdm(val_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            input_ids = batch[0].to(\"cuda\")\n",
        "            target_ids = batch[1].to(\"cuda\")\n",
        "        else:\n",
        "            input_ids = batch[0]\n",
        "            target_ids = batch[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(input_ids)\n",
        "\n",
        "        predictions = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in outputs]\n",
        "        targets = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in target_ids]\n",
        "\n",
        "        all_predictions.extend(predictions)\n",
        "        all_targets.extend(targets)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_targets, all_predictions)\n",
        "    precision = precision_score(all_targets, all_predictions, average='weighted')\n",
        "    recall = recall_score(all_targets, all_predictions, average='weighted')\n",
        "    f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
        "\n",
        "    print(f\"Fold {fold + 1} Validation Metrics: \")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "\n",
        "    fold_metrics['accuracy'].append(accuracy)\n",
        "    fold_metrics['precision'].append(precision)\n",
        "    fold_metrics['recall'].append(recall)\n",
        "    fold_metrics['f1_score'].append(f1)\n",
        "\n",
        "    # Store fold metrics\n",
        "    all_fold_metrics.append(fold_metrics)\n",
        "\n",
        "# Averaging the metrics over all folds\n",
        "average_metrics = {key: np.mean([fold_metrics[key] for fold_metrics in all_fold_metrics]) for key in fold_metrics.keys()}\n",
        "print(\"Average Metrics Across All Folds: \", average_metrics)\n"
      ],
      "metadata": {
        "id": "cK4-OFGz0fJz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}